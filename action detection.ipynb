{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29578aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d666c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models,layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3ca515",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7950a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4a251f-83c3-4aaa-8239-1ac50a924e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmark(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
    "                             )\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121),thickness=1,circle_radius=1)\n",
    "                             )\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250),thickness=1,circle_radius=1)\n",
    "                             )\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230),thickness=1,circle_radius=1)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04043f6-d786-41cd-90be-9525f34e54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_count = 0\n",
    "# fps = 30\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# out = cv2.VideoWriter('D:/Research_Paper/Sign_detection/Model/output.mp4', fourcc, fps, (640, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcc0400-d26d-4540-adfb-5027c7a6c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the loop\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()  # Read the frame from the webcam\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         image, results = mediapipe_detection(frame, holistic)  # Detect landmarks\n",
    "\n",
    "#         draw_styled_landmark(image, results)  # Draw landmarks on the frame\n",
    "\n",
    "#         out.write(image)  # Write the frame to the video file\n",
    "\n",
    "#         cv2.imshow('OvenCv Feed', image)  # Display the frame\n",
    "\n",
    "#         frame_count += 1  # Increment the frame count\n",
    "\n",
    "#         if frame_count == fps * 5:  # If 5 seconds have passed\n",
    "#             break\n",
    "\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):  # If 'q' is pressed\n",
    "#             break\n",
    "\n",
    "#     cap.release()  # Release the webcam\n",
    "#     out.release()  # Release the video writer\n",
    "#     cv2.destroyAllWindows()  # Close all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e8e293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "040d019f-ad82-4a5b-8b15-1eff51a6fbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf2628fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# def capture_and_save_video(output_file=\"D:/Research_Paper/Sign_detection/Videos/output.mp4\", duration=5):\n",
    "#     # Open a video capture object (0 indicates the default camera)\n",
    "#     with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "#         cap = cv2.VideoCapture(0)\n",
    "#     # Check if the camera opened successfully\n",
    "#         if not cap.isOpened():\n",
    "#             print(\"Error: Could not open camera.\")\n",
    "#             return\n",
    "        \n",
    "#         # Get the frames per second (fps) of the camera\n",
    "#         fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "#         # Create a VideoWriter object to save the video\n",
    "#         fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Specify codec\n",
    "#         out = cv2.VideoWriter(output_file, fourcc, fps, (640, 480))  # Adjust resolution as needed\n",
    "        \n",
    "#         # Capture video for the specified duration\n",
    "#         start_time = cv2.getTickCount() / cv2.getTickFrequency()\n",
    "#         while (cv2.getTickCount() / cv2.getTickFrequency()) - start_time < duration:\n",
    "#             ret, frame = cap.read()  # Read a frame\n",
    "#             if not ret:\n",
    "#                 print(\"Error: Failed to capture a frame.\")\n",
    "#                 break\n",
    "#             image, results = mediapipe_detection(frame,holistic)\n",
    "#             draw_styled_landmark(image,results)\n",
    "#             # Display the frame\n",
    "#             cv2.imshow('Video Capture', frame)\n",
    "        \n",
    "#             # Write the frame to the output video file\n",
    "#             out.write(frame)\n",
    "        \n",
    "#             # Break the loop if 'q' key is pressed\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "#         cap.release()\n",
    "#         out.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "# capture_and_save_video()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1aa2f8-5415-4b47-894f-c2457da3a21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2171b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_landmark(image, results):\n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "#     mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "#     mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "#     mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_hands.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b38b4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5239a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 19, 148, 148, 32   2624      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3  (None, 9, 74, 74, 32)     0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 7, 72, 72, 32)     27680     \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPoolin  (None, 3, 36, 36, 32)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 2, 35, 35, 64)     16448     \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPoolin  (None, 1, 17, 17, 64)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18496)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 18496)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 500)            37994000  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1, 100)            240400    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50)                30200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 48)                2448      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38313800 (146.16 MB)\n",
      "Trainable params: 38313800 (146.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "frame_shape=(21,150,150, 3)\n",
    "model = models.Sequential([\n",
    "\n",
    "  Conv3D(32, (3,3,3), activation='relu', input_shape=frame_shape),\n",
    "  MaxPooling3D(pool_size = 2),\n",
    "\n",
    "  Conv3D(32 , kernel_size = (3,3,3) ,activation = 'relu',),\n",
    "  MaxPooling3D(pool_size=2),\n",
    "\n",
    "  Conv3D(64 , kernel_size = (2,2,2) ,activation = 'relu',),\n",
    "  MaxPooling3D(pool_size=2),\n",
    "  # Conv3D(64 , kernel_size = (2,2,2) ,activation = 'relu',),\n",
    "  # MaxPooling3D(pool_size=2),\n",
    "\n",
    "  #   layers.Conv3D(64 , kernel_size = (3,3,3) ,activation = 'relu',),\n",
    "  #   layers.MaxPooling3D(pool_size=2),\n",
    "\n",
    "  #   layers.Conv3D(64 , kernel_size = (3,3,3) ,activation = 'relu',),\n",
    "  #   layers.MaxPooling3D(pool_size=2),\n",
    "\n",
    "  Flatten(),\n",
    " layers.Reshape((1,18496), input_shape=(18496,)),\n",
    " LSTM(500, return_sequences=True),\n",
    " LSTM(100, return_sequences= True),\n",
    "LSTM(50,return_sequences=False),\n",
    " Dense(num_classes, activation='softmax')\n",
    " ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cc4f647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35df7d71-10e0-4d03-b90b-002b7d511fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('C:/Users/manu/Downloads/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df0820fa-7924-49c2-b6a4-4fd9a6015ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_frames(video_path, num_frames=30, fps=24):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     if not cap.isOpened():\n",
    "#         raise ValueError(f\"Error opening video: {video_path}\")\n",
    "\n",
    "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     # print(\"frame_count\",frame_count)\n",
    "#     # print(\"num_frames\",num_frames)\n",
    "#     if frame_count < 1:\n",
    "#         raise ValueError(\"Video has no frames\")\n",
    "\n",
    "#     # Ensure at least one frame is extracted, even for short videos\n",
    "#     if num_frames < 1:\n",
    "#         raise ValueError(\"Number of frames must be at least 1\")\n",
    "\n",
    "#     # print(\"FPS\",int(cap.get(cv2.CAP_PROP_FPS)) )\n",
    "\n",
    "#     frame_interval = max(1, int(cap.get(cv2.CAP_PROP_FPS)) // num_frames)\n",
    "#     # print(\"Frame_interval\",frame_interval)\n",
    "#     frames = []\n",
    "#     count = 0\n",
    "#     while cap.isOpened():\n",
    "#       if count <= 20:\n",
    "#           # print(count)\n",
    "#           ret, frame = cap.read()\n",
    "#           if not ret:\n",
    "#               #if ret == False\n",
    "#               # print(\"Ret value\",ret)\n",
    "#               break\n",
    "#           if count % frame_interval == 0:\n",
    "#               frames.append(frame)\n",
    "#           count += 1\n",
    "#       elif count > 20:\n",
    "#           cap.release()\n",
    "#     if not frames:\n",
    "#         print(\"Warning: No frames extracted from video.\")\n",
    "#         return []  # Return an empty list\n",
    "#     return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "843238ba-2b89-452c-8a0b-11fe707a69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(frames, resize_shape=(150, 150)):\n",
    "\n",
    "    preprocessed_frames = np.empty((len(frames), *resize_shape, 3))\n",
    "    for i, frame in enumerate(frames):\n",
    "        frame = cv2.resize(frame, resize_shape)\n",
    "        frame = frame / 255.0  # Normalize pixel values to [0, 1]\n",
    "        preprocessed_frames[i] = frame\n",
    "\n",
    "    return preprocessed_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325fd873-96c6-415f-91b4-3d8123be2c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c92c1db9-1cf2-411e-bd4b-ebf5f2c36bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_video_with_landmarks():\n",
    "    # Initialize MediaPipe Holistic\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    holistic = mp_holistic.Holistic()\n",
    "\n",
    "    # Initialize OpenCV VideoCapture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(\"D:/Research_Paper/Sign_detection/Videos/output.mp4\", fourcc, 20.0, (640, 480))\n",
    "\n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Flip the frame horizontally for a later selfie-view display\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Holistic\n",
    "        results = holistic.process(rgb_frame)\n",
    "\n",
    "        # Draw landmarks on the frame\n",
    "        if results.pose_landmarks:\n",
    "            draw_styled_landmark(frame, results)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # Break the loop if the time limit is reached\n",
    "        if time.time() - start_time > 5:\n",
    "            break\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Close MediaPipe Holistic\n",
    "    holistic.close()\n",
    "\n",
    "# Call the function to capture the video with landmarks\n",
    "capture_video_with_landmarks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "10dbc0b5-d172-44dc-a2fa-9f6ffa3a6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, sequence_length, num_classes):\n",
    "  X= []\n",
    "  frames = preprocess_frames(extract_frames(\"D:/Research_Paper/Sign_detection/Videos/output.mp4\"))\n",
    "  # print(\"Frames\",frames,end = \"\\n\")\n",
    "            # Create sequence\n",
    "  for i in range(len(frames) - sequence_length + 1):\n",
    "      sequence = frames[i:i + sequence_length]\n",
    "              # print(f\"{g}Sequence\", sequence,end=\"\\n\")\n",
    "      X.append(sequence)\n",
    "  print(\"done\")\n",
    "  # print(X)\n",
    "  re = np.array(X)\n",
    "  # print(re)\n",
    "  print(\"Shape of X -: \",re.shape)\n",
    "  return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2733bb72-216d-4fe6-af05-c8dc909fa148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Shape of X -:  (1, 21, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "data = load_data(\"D:/Research_Paper/Sign_detection/Videos/\",21,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a562a-b6b4-4b02-a65b-04160f4bfe45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3c3997d9-257a-4e69-9c57-aae682f30da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(['you', 'you', 'you', 'you', 'you', 'you', 'your', 'your', 'your', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'wrong', 'wrong', 'wrong', 'wrong', 'wrong', 'who', 'who', 'who', 'who', 'who', 'who', 'see you letter', 'see you letter', 'see you letter', 'see you letter', 'see you letter', 'see you letter', 'see you letter', 'see you letter', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'want', 'want', 'want', 'want', 'want', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'thank you', 'thank you', 'thank you', 'thank you', 'thank you', 'thank you', 'thank you', 'thank you', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'same', 'same', 'same', 'same', 'same', 'same', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'no', 'no', 'no', 'no', 'no', 'no', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'question', 'question', 'question', 'question', 'question', 'question', 'question', 'name', 'name', 'name', 'name', 'name', 'name', 'name', 'name', 'name', 'name', 'name', 'more', 'more', 'more', 'more', 'more', 'more', 'more', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'milk', 'milk', 'milk', 'milk', 'milk', 'milk', 'milk', 'milk', 'nice', 'nice', 'nice', 'nice', 'nice', 'nice', 'nice', 'please', 'please', 'please', 'please', 'please', 'please', 'please', 'need', 'need', 'need', 'need', 'need', 'need', 'need', 'need', 'need', 'go', 'go', 'go', 'go', 'go', 'go', 'go', 'meet', 'meet', 'meet', 'meet', 'meet', 'meet', 'meet', 'meet', 'meet', 'i', 'i', 'i', 'i', 'i', 'i', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'help', 'help', 'help', 'help', 'help', 'help', 'help', 'good', 'good', 'good', 'good', 'good', 'good', 'good', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'learn', 'learn', 'learn', 'learn', 'learn', 'learn', 'learn', 'learn', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'bad', 'bad', 'bad', 'bad', 'bad', 'bad', 'bad', 'busy', 'busy', 'busy', 'busy', 'busy', 'busy', 'finish', 'finish', 'finish', 'finish', 'finish', 'finish', 'finish', 'fine', 'fine', 'fine', 'fine', 'fine', 'fine', 'fine', 'father', 'father', 'father', 'father', 'father', 'father', 'bathroom', 'bathroom', 'bathroom', 'bathroom', 'bathroom', 'bathroom', 'bathroom', 'bathroom', 'bathroom', 'book', 'book', 'book', 'book', 'book', 'book', 'book', 'do not want', 'do not want', 'do not want', 'do not want', 'do not want', 'do not want', 'forget', 'forget', 'forget', 'forget', 'forget', 'forget', 'forget', 'again', 'again', 'again', 'again', 'again', 'again', 'again', 'again'])\n",
    "\n",
    "# Convert the NumPy array to a list\n",
    "y_list = y.tolist()\n",
    "unique_elements = list(set(y))\n",
    "dict = {element: i + 1 for i, element in enumerate(unique_elements)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2833f68c-9ee7-4ee7-b135-5058002908dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_key_by_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "105f3388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 208ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fine'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_key_by_value(dict,(np.argmax(model.predict(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203531a-73d0-41ab-b903-c56ce0349aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f262d04c-4def-4974-9842-a9d6ccb38661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd6474-8ae5-4a0d-a336-c9c397eefb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d7a9b-1b35-451c-aa16-25a7f52057b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "35ea92b8-846a-454a-bb1c-1959930aea9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71076b31-6515-49b0-a54d-439d8c491221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a589090-e405-41bb-9a52-a5d42313d8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a126f87-1a79-4604-a9c9-dabdecae9d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea7952-fc29-4011-8924-db2baac3a21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b5f13-fdc8-4cb7-90b5-0b02bc99099a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644fd28-cc5a-4243-a05d-012b3cd8edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464e78e-535b-46db-9746-2954ea2135a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
